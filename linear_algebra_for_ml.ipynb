{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns\n",
    "from cmath import sqrt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Vectors & Basic Operations\n",
    "\n",
    "### What is a Vector?\n",
    "In ML, vectors represent:\n",
    "- **Feature vectors**: Each data point (e.g., [height, weight, age])\n",
    "- **Model parameters**: Weights in linear regression\n",
    "- **Embeddings**: Word vectors, image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student 1 features: [170  65  20]\n",
      "Student 2 features: [180  75  22]\n",
      "Vector shape: (3,)\n",
      "Vector dimension: 3\n"
     ]
    }
   ],
   "source": [
    "# Creating vectors in NumPy\n",
    "# Example: Student data [height_cm, weight_kg, age_years]\n",
    "student_1 = np.array([170, 65, 20])\n",
    "student_2 = np.array([180, 75, 22])\n",
    "\n",
    "print(f\"Student 1 features: {student_1}\")\n",
    "print(f\"Student 2 features: {student_2}\")\n",
    "print(f\"Vector shape: {student_1.shape}\")\n",
    "print(f\"Vector dimension: {len(student_1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Vector Operations\n",
    "\n",
    "#### 1. Dot Product (Scalar Product)\n",
    "**Mathematical Definition**: $\\vec{a} \\cdot \\vec{b} = \\sum_{i=1}^{n} a_i b_i$\n",
    "\n",
    "**ML Applications**:\n",
    "- Similarity measurement\n",
    "- Linear regression predictions\n",
    "- Neural network computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.1: Implement dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual dot product: 35915\n",
      "NumPy dot product: 35915\n",
      "Alternative syntax: 35915\n"
     ]
    }
   ],
   "source": [
    "def dot_product_manual(a, b):\n",
    "    \"\"\"Compute dot product manually\"\"\"\n",
    "    # TODO: Implement using a loop\n",
    "    result = 0\n",
    "    for i in range(len(a)):\n",
    "        result += a[i] * b[i]\n",
    "    return result\n",
    "\n",
    "# Compare with NumPy\n",
    "manual_result = dot_product_manual(student_1, student_2)\n",
    "numpy_result = np.dot(student_1, student_2)\n",
    "\n",
    "print(f\"Manual dot product: {manual_result}\")\n",
    "print(f\"NumPy dot product: {numpy_result}\")\n",
    "print(f\"Alternative syntax: {student_1 @ student_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Vector Norms (Magnitude)\n",
    "**L2 Norm (Euclidean)**: $||\\vec{v}||_2 = \\sqrt{\\sum_{i=1}^{n} v_i^2}$\n",
    "\n",
    "**ML Applications**:\n",
    "- Feature normalization\n",
    "- Distance calculations\n",
    "- Regularization in models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2: Vector norms and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm of student 1: 183.10\n",
      "Normalized student 1: [0.92846284 0.3550005  0.10923092]\n",
      "Norm of normalized vector: 1.000000\n",
      "L1 norm of student 1: 255\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# L2 norm (Euclidean distance from origin)\n",
    "norm_student_1 = np.linalg.norm(student_1)\n",
    "print(f\"L2 norm of student 1: {norm_student_1:.2f}\")\n",
    "\n",
    "# Normalize vector (unit vector)\n",
    "student_1_normalized = student_1 / norm_student_1\n",
    "print(f\"Normalized student 1: {student_1_normalized}\")\n",
    "print(f\"Norm of normalized vector: {np.linalg.norm(student_1_normalized):.6f}\")\n",
    "\n",
    "# TODO: Calculate L1 norm (Manhattan distance)\n",
    "l1_norm = np.sum(np.abs(student_1))\n",
    "print(f\"L1 norm of student 1: {l1_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183.09833423600554\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "sum = 0\n",
    "for i in student_1:\n",
    "    sum += i*i\n",
    "\n",
    "print(sum**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Cosine Similarity\n",
    "**Formula**: $\\text{cosine similarity} = \\frac{\\vec{a} \\cdot \\vec{b}}{||\\vec{a}|| ||\\vec{b}||}$\n",
    "\n",
    "**Range**: [-1, 1] where 1 = identical direction, 0 = perpendicular, -1 = opposite\n",
    "\n",
    "**ML Applications**: Document similarity, recommendation systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.3: Implement cosine similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norms = np.linalg.norm(a) * np.linalg.norm(b)\n",
    "    return dot_product / norms\n",
    "\n",
    "# Example: Compare students\n",
    "similarity = cosine_similarity(student_1, student_2)\n",
    "print(f\"Cosine similarity between students: {similarity:.4f}\")\n",
    "\n",
    "# Visualize vectors\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# 2D projection for visualization (using first 2 features)\n",
    "ax1.quiver(0, 0, student_1[0], student_1[1], angles='xy', scale_units='xy', scale=1, color='red', label='Student 1')\n",
    "ax1.quiver(0, 0, student_2[0], student_2[1], angles='xy', scale_units='xy', scale=1, color='blue', label='Student 2')\n",
    "ax1.set_xlim(0, 200)\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.set_xlabel('Height (cm)')\n",
    "ax1.set_ylabel('Weight (kg)')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "ax1.set_title('Vector Visualization (2D Projection)')\n",
    "\n",
    "# Bar plot of all features\n",
    "x = ['Height', 'Weight', 'Age']\n",
    "width = 0.35\n",
    "x_pos = np.arange(len(x))\n",
    "ax2.bar(x_pos - width/2, student_1, width, label='Student 1', alpha=0.8)\n",
    "ax2.bar(x_pos + width/2, student_2, width, label='Student 2', alpha=0.8)\n",
    "ax2.set_xlabel('Features')\n",
    "ax2.set_ylabel('Values')\n",
    "ax2.set_title('Feature Comparison')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(x)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beginner Exercises: Building Foundation\n",
    "\n",
    "Let's practice basic vector operations that form the foundation of ML algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Given two feature vectors representing houses\n",
    "house_1 = np.array([1200, 3, 2, 15])  # [sqft, bedrooms, bathrooms, age_years]\n",
    "house_2 = np.array([1800, 4, 3, 5])\n",
    "\n",
    "# TODO: Calculate the following\n",
    "# 1. Euclidean distance between houses\n",
    "distance = # Your code here\n",
    "\n",
    "# 2. Cosine similarity\n",
    "similarity = # Your code here\n",
    "\n",
    "# 3. Normalize both house vectors\n",
    "house_1_norm = # Your code here\n",
    "house_2_norm = # Your code here\n",
    "\n",
    "print(f\"Distance: {distance:.2f}\")\n",
    "print(f\"Similarity: {similarity:.4f}\")\n",
    "print(f\"House 1 normalized: {house_1_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.5: ðŸŸ¢ Vector Addition and Subtraction\n",
    "#### Learning Objective: Understand vector arithmetic and geometric interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Two motion vectors representing displacement\n",
    "motion_1 = np.array([3, 4])  # 3 units right, 4 units up\n",
    "motion_2 = np.array([-1, 2]) # 1 unit left, 2 units up\n",
    "\n",
    "print(\"Vector Operations:\")\n",
    "print(f\"Motion 1: {motion_1}\")\n",
    "print(f\"Motion 2: {motion_2}\")\n",
    "\n",
    "# TODO: Calculate vector operations\n",
    "vector_sum = # Your code here\n",
    "vector_diff = # Your code here\n",
    "scalar_mult = # Your code here (multiply motion_1 by 2)\n",
    "\n",
    "print(f\"\\nVector sum (combined motion): {vector_sum}\")\n",
    "print(f\"Vector difference: {vector_diff}\")\n",
    "print(f\"Scalar multiplication (2 * motion_1): {scalar_mult}\")\n",
    "\n",
    "# Visualize the vectors\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot vectors from origin\n",
    "plt.quiver(0, 0, motion_1[0], motion_1[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='red', width=0.005, label='Motion 1')\n",
    "plt.quiver(0, 0, motion_2[0], motion_2[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='blue', width=0.005, label='Motion 2')\n",
    "plt.quiver(0, 0, vector_sum[0], vector_sum[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='green', width=0.007, label='Sum')\n",
    "\n",
    "# Plot vector addition graphically (tip-to-tail)\n",
    "plt.quiver(motion_1[0], motion_1[1], motion_2[0], motion_2[1], angles='xy', scale_units='xy', \n",
    "           scale=1, color='blue', width=0.003, alpha=0.5)\n",
    "\n",
    "plt.xlim(-2, 6)\n",
    "plt.ylim(-1, 7)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Vector Addition: Geometric Interpretation')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# Quick check\n",
    "print(f\"\\nGeometric verification:\")\n",
    "print(f\"Length of motion_1: {np.linalg.norm(motion_1):.2f}\")\n",
    "print(f\"Length of motion_2: {np.linalg.norm(motion_2):.2f}\")\n",
    "print(f\"Length of sum: {np.linalg.norm(vector_sum):.2f}\")\n",
    "print(f\"Triangle inequality satisfied: {np.linalg.norm(vector_sum) <= np.linalg.norm(motion_1) + np.linalg.norm(motion_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.6: ðŸŸ¢ Finding Angles Between Vectors\n",
    "#### Learning Objective: Understand geometric relationships in vector spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Two user preference vectors for movie genres\n",
    "user_A_preferences = np.array([5, 2, 1, 4])  # [Action, Comedy, Drama, Sci-Fi]\n",
    "user_B_preferences = np.array([2, 5, 4, 1])\n",
    "user_C_preferences = np.array([5, 2, 1, 4])  # Same as A\n",
    "\n",
    "print(\"User Movie Preferences (1-5 scale):\")\n",
    "print(f\"User A: {user_A_preferences}\")\n",
    "print(f\"User B: {user_B_preferences}\")\n",
    "print(f\"User C: {user_C_preferences}\")\n",
    "\n",
    "def angle_between_vectors(v1, v2):\n",
    "    \\\"\\\"\\\"Calculate angle between two vectors in degrees\\\"\\\"\\\"\n",
    "    # TODO: Implement angle calculation\n",
    "    # Hint: cos(Î¸) = (aÂ·b) / (|a||b|)\n",
    "    # Use np.arccos and np.degrees\n",
    "    \n",
    "    cos_angle = # Your code here\n",
    "    angle_rad = # Your code here\n",
    "    angle_deg = # Your code here\n",
    "    \n",
    "    return angle_deg\n",
    "\n",
    "# Calculate angles\n",
    "angle_AB = angle_between_vectors(user_A_preferences, user_B_preferences)\n",
    "angle_AC = angle_between_vectors(user_A_preferences, user_C_preferences)\n",
    "angle_BC = angle_between_vectors(user_B_preferences, user_C_preferences)\n",
    "\n",
    "print(f\"\\nAngles between user preferences:\")\n",
    "print(f\"A-B: {angle_AB:.1f}Â°\")\n",
    "print(f\"A-C: {angle_AC:.1f}Â°\")\n",
    "print(f\"B-C: {angle_BC:.1f}Â°\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"- Small angle (< 30Â°): Very similar preferences\")\n",
    "print(f\"- Medium angle (30-60Â°): Somewhat similar\")\n",
    "print(f\"- Large angle (60-90Â°): Different preferences\")  \n",
    "print(f\"- 90Â°: Orthogonal (no correlation)\")\n",
    "print(f\"- > 90Â°: Opposite preferences\")\n",
    "\n",
    "# Visualize in 2D (using first two genres)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.quiver(0, 0, user_A_preferences[0], user_A_preferences[1], angles='xy', \n",
    "           scale_units='xy', scale=1, color='red', width=0.01, label='User A')\n",
    "plt.quiver(0, 0, user_B_preferences[0], user_B_preferences[1], angles='xy', \n",
    "           scale_units='xy', scale=1, color='blue', width=0.01, label='User B')\n",
    "plt.quiver(0, 0, user_C_preferences[0], user_C_preferences[1], angles='xy', \n",
    "           scale_units='xy', scale=1, color='green', width=0.01, label='User C')\n",
    "\n",
    "plt.xlim(0, 6)\n",
    "plt.ylim(0, 6)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('Action Movies (Rating)')\n",
    "plt.ylabel('Comedy Movies (Rating)')\n",
    "plt.title('User Preferences: 2D Projection')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# Challenge: What does a 90-degree angle mean for user preferences?\n",
    "print(f\"\\n Think: If two users have a 90Â° angle between their preference vectors,\")\n",
    "print(f\"what does this tell us about their movie tastes?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.7: ðŸŸ¢ Different Norms and Their Meanings\n",
    "#### Learning Objective: Understand various distance metrics and their applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sales performance vector: [Q1, Q2, Q3, Q4] sales in thousands\n",
    "salesperson_A = np.array([120, 80, 150, 90])\n",
    "salesperson_B = np.array([100, 110, 100, 130])\n",
    "salesperson_C = np.array([200, 30, 50, 160])\n",
    "\n",
    "print(\"Quarterly Sales Performance (in thousands):\")\n",
    "print(f\"Salesperson A: {salesperson_A}\")\n",
    "print(f\"Salesperson B: {salesperson_B}\")\n",
    "print(f\"Salesperson C: {salesperson_C}\")\n",
    "\n",
    "def calculate_norms(vector):\n",
    "    \\\"\\\"\\\"Calculate different types of norms\\\"\\\"\\\"\n",
    "    # TODO: Calculate different norms\n",
    "    l1_norm = # Your code here (Manhattan/Taxicab distance)\n",
    "    l2_norm = # Your code here (Euclidean distance)\n",
    "    l_inf_norm = # Your code here (Maximum/Chebyshev distance)x`\n",
    "    \n",
    "    return l1_norm, l2_norm, l_inf_norm\n",
    "\n",
    "# Calculate norms for each salesperson\n",
    "norms_A = calculate_norms(salesperson_A)\n",
    "norms_B = calculate_norms(salesperson_B)\n",
    "norms_C = calculate_norms(salesperson_C)\n",
    "\n",
    "print(f\"\\nNorm Comparison:\")\n",
    "print(f\"{'Salesperson':<12} {'L1 (Sum)':<12} {'L2 (Euclidean)':<15} {'Lâˆž (Max)':<12}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{'A':<12} {norms_A[0]:<12.1f} {norms_A[1]:<15.1f} {norms_A[2]:<12.1f}\")\n",
    "print(f\"{'B':<12} {norms_B[0]:<12.1f} {norms_B[1]:<15.1f} {norms_B[2]:<12.1f}\")\n",
    "print(f\"{'C':<12} {norms_C[0]:<12.1f} {norms_C[1]:<15.1f} {norms_C[2]:<12.1f}\")\n",
    "\n",
    "# Visualize the different norms\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "quarters = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "width = 0.25\n",
    "x = np.arange(len(quarters))\n",
    "\n",
    "# Sales comparison\n",
    "axes[0].bar(x - width, salesperson_A, width, label='A', alpha=0.8)\n",
    "axes[0].bar(x, salesperson_B, width, label='B', alpha=0.8)\n",
    "axes[0].bar(x + width, salesperson_C, width, label='C', alpha=0.8)\n",
    "axes[0].set_xlabel('Quarter')\n",
    "axes[0].set_ylabel('Sales (thousands)')\n",
    "axes[0].set_title('Quarterly Sales')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(quarters)\n",
    "axes[0].legend()\n",
    "\n",
    "# Norm comparison\n",
    "norm_types = ['L1', 'L2', 'Lâˆž']\n",
    "norm_values_A = list(norms_A)\n",
    "norm_values_B = list(norms_B)\n",
    "norm_values_C = list(norms_C)\n",
    "\n",
    "x_norms = np.arange(len(norm_types))\n",
    "axes[1].bar(x_norms - width, norm_values_A, width, label='A', alpha=0.8)\n",
    "axes[1].bar(x_norms, norm_values_B, width, label='B', alpha=0.8)\n",
    "axes[1].bar(x_norms + width, norm_values_C, width, label='C', alpha=0.8)\n",
    "axes[1].set_xlabel('Norm Type')\n",
    "axes[1].set_ylabel('Norm Value')\n",
    "axes[1].set_title('Different Norms Comparison')\n",
    "axes[1].set_xticks(x_norms)\n",
    "axes[1].set_xticklabels(norm_types)\n",
    "axes[1].legend()\n",
    "\n",
    "# Ranking by each norm\n",
    "l1_ranking = ['A', 'B', 'C'][np.argsort([norms_A[0], norms_B[0], norms_C[0]])[::-1]]\n",
    "l2_ranking = ['A', 'B', 'C'][np.argsort([norms_A[1], norms_B[1], norms_C[1]])[::-1]]\n",
    "linf_ranking = ['A', 'B', 'C'][np.argsort([norms_A[2], norms_B[2], norms_C[2]])[::-1]]\n",
    "\n",
    "axes[2].text(0.1, 0.8, f\"L1 Ranking:\\\\n1st: {l1_ranking[0]}\\\\n2nd: {l1_ranking[1]}\\\\n3rd: {l1_ranking[2]}\", \n",
    "             transform=axes[2].transAxes, fontsize=10, verticalalignment='top')\n",
    "axes[2].text(0.1, 0.5, f\"L2 Ranking:\\\\n1st: {l2_ranking[0]}\\\\n2nd: {l2_ranking[1]}\\\\n3rd: {l2_ranking[2]}\", \n",
    "             transform=axes[2].transAxes, fontsize=10, verticalalignment='top')\n",
    "axes[2].text(0.1, 0.2, f\"Lâˆž Ranking:\\\\n1st: {linf_ranking[0]}\\\\n2nd: {linf_ranking[1]}\\\\n3rd: {linf_ranking[2]}\", \n",
    "             transform=axes[2].transAxes, fontsize=10, verticalalignment='top')\n",
    "axes[2].set_title('Performance Rankings by Norm')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Norm Interpretations:\")\n",
    "print(f\"â€¢ L1 (Manhattan): Total sales volume - measures overall productivity\")\n",
    "print(f\"â€¢ L2 (Euclidean): 'Natural' distance - balanced performance measure\") \n",
    "print(f\"â€¢ Lâˆž (Maximum): Best quarter - measures peak performance\")\n",
    "print(f\"\\nðŸ’¡ Which norm would you use to evaluate salespeople and why?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.8: ðŸŸ¡ Vector Projections and Components\n",
    "#### Learning Objective: Understand projections for dimensionality reduction and feature extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Weather data: [temperature, pressure] and wind vector [wind_x, wind_y]\n",
    "weather_data = np.array([25, 1015])  # 25Â°C, 1015 hPa\n",
    "wind_direction = np.array([1, 0])    # East direction (unit vector)\n",
    "wind_vector = np.array([8, 6])       # 8 m/s East, 6 m/s North\n",
    "\n",
    "print(\"Weather Analysis:\")\n",
    "print(f\"Weather conditions: {weather_data} [Â°C, hPa]\")\n",
    "print(f\"Wind vector: {wind_vector} [m/s East, m/s North]\")\n",
    "print(f\"Reference direction (East): {wind_direction}\")\n",
    "\n",
    "def vector_projection(vector, onto):\n",
    "    \\\"\\\"\\\"Project vector onto another vector\\\"\\\"\\\"\n",
    "    # TODO: Implement vector projection\n",
    "    # Formula: proj_onto(vector) = (vector Â· onto / |onto|Â²) * onto\n",
    "    \n",
    "    dot_product = # Your code here\n",
    "    onto_squared = # Your code here  \n",
    "    projection = # Your code here\n",
    "    \n",
    "    return projection\n",
    "\n",
    "def orthogonal_component(vector, onto):\n",
    "    \\\"\\\"\\\"Find the component orthogonal to the projection\\\"\\\"\\\"\n",
    "    # TODO: Implement orthogonal component\n",
    "    # Hint: orthogonal = original - projection\n",
    "    \n",
    "    proj = vector_projection(vector, onto)\n",
    "    orthogonal = # Your code here\n",
    "    \n",
    "    return orthogonal\n",
    "\n",
    "# Project wind onto east direction\n",
    "wind_east_component = vector_projection(wind_vector, wind_direction)\n",
    "wind_north_component = orthogonal_component(wind_vector, wind_direction)\n",
    "\n",
    "print(f\"\\nWind Analysis:\")\n",
    "print(f\"East component (projection): {wind_east_component}\")\n",
    "print(f\"North component (orthogonal): {wind_north_component}\")\n",
    "print(f\"East speed: {np.linalg.norm(wind_east_component):.1f} m/s\")\n",
    "print(f\"North speed: {np.linalg.norm(wind_north_component):.1f} m/s\")\n",
    "\n",
    "# Verify orthogonality\n",
    "dot_check = np.dot(wind_east_component, wind_north_component)\n",
    "print(f\"Orthogonality check (should be ~0): {dot_check:.6f}\")\n",
    "\n",
    "# Verify reconstruction\n",
    "reconstructed = wind_east_component + wind_north_component\n",
    "reconstruction_error = np.linalg.norm(wind_vector - reconstructed)\n",
    "print(f\"Reconstruction error: {reconstruction_error:.6f}\")\n",
    "\n",
    "# Visualize projections\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Original vectors\n",
    "plt.quiver(0, 0, wind_vector[0], wind_vector[1], angles='xy', scale_units='xy', \n",
    "           scale=1, color='blue', width=0.01, label='Wind Vector', linewidth=2)\n",
    "plt.quiver(0, 0, wind_direction[0]*10, wind_direction[1]*10, angles='xy', scale_units='xy', \n",
    "           scale=1, color='black', width=0.005, label='East Direction', alpha=0.7)\n",
    "\n",
    "# Projections\n",
    "plt.quiver(0, 0, wind_east_component[0], wind_east_component[1], angles='xy', scale_units='xy', \n",
    "           scale=1, color='red', width=0.008, label='East Component')\n",
    "plt.quiver(wind_east_component[0], wind_east_component[1], \n",
    "           wind_north_component[0], wind_north_component[1], angles='xy', scale_units='xy', \n",
    "           scale=1, color='green', width=0.008, label='North Component')\n",
    "\n",
    "# Projection line\n",
    "plt.plot([wind_vector[0], wind_east_component[0]], \n",
    "         [wind_vector[1], wind_east_component[1]], \n",
    "         'r--', alpha=0.5, label='Projection Line')\n",
    "\n",
    "plt.xlim(-1, 10)\n",
    "plt.ylim(-1, 8)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('East (m/s)')\n",
    "plt.ylabel('North (m/s)')\n",
    "plt.title('Vector Projection: Wind Components')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# ML Application: Feature extraction\n",
    "print(f\"\\n ML Application:\")\n",
    "print(f\"In PCA, we project high-dimensional data onto principal components\")\n",
    "print(f\"This is exactly what we did - projecting the 2D wind vector onto 1D!\")\n",
    "print(f\"The projection captures the 'main direction' of the wind.\")\n",
    "\n",
    "# Challenge question\n",
    "print(f\"\\n Challenge: If you were building a wind turbine, would you care more\")\n",
    "print(f\"about the east component or the total wind speed? Why?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.9: ðŸŸ¡ Building Similarity Matrices\n",
    "#### Learning Objective: Create pairwise similarity computations for recommendation systems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Student skill profiles: [Math, Programming, Communication, Creativity, Leadership]\n",
    "students = {\n",
    "    'Alice': np.array([9, 8, 6, 7, 5]),\n",
    "    'Bob': np.array([7, 9, 5, 8, 6]), \n",
    "    'Carol': np.array([8, 6, 9, 6, 8]),\n",
    "    'David': np.array([6, 7, 8, 9, 7]),\n",
    "    'Eve': np.array([9, 8, 7, 6, 9])\n",
    "}\n",
    "\n",
    "skill_names = ['Math', 'Programming', 'Communication', 'Creativity', 'Leadership']\n",
    "student_names = list(students.keys())\n",
    "student_vectors = np.array(list(students.values()))\n",
    "\n",
    "print(\"Student Skill Profiles (1-10 scale):\")\n",
    "for name, skills in students.items():\n",
    "    print(f\"{name:6}: {skills}\")\n",
    "\n",
    "def compute_similarity_matrix(vectors, metric='cosine'):\n",
    "    \\\"\\\"\\\"Compute pairwise similarity matrix\\\"\\\"\\\"\n",
    "    n = len(vectors)\n",
    "    similarity_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if metric == 'cosine':\n",
    "                # TODO: Implement cosine similarity\n",
    "                similarity = # Your code here\n",
    "            elif metric == 'correlation':\n",
    "                # TODO: Implement Pearson correlation\n",
    "                # Hint: Use np.corrcoef or implement manually\n",
    "                similarity = # Your code here\n",
    "            elif metric == 'euclidean':\n",
    "                # TODO: Convert distance to similarity\n",
    "                # Hint: similarity = 1 / (1 + distance)\n",
    "                distance = # Your code here\n",
    "                similarity = # Your code here\n",
    "            \n",
    "            similarity_matrix[i, j] = similarity\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "# Compute different similarity matrices\n",
    "cosine_sim = compute_similarity_matrix(student_vectors, 'cosine')\n",
    "correlation_sim = compute_similarity_matrix(student_vectors, 'correlation')\n",
    "euclidean_sim = compute_similarity_matrix(student_vectors, 'euclidean')\n",
    "\n",
    "print(f\"\\nSimilarity Matrices:\")\n",
    "\n",
    "# Display cosine similarity\n",
    "print(f\"\\nCosine Similarity:\")\n",
    "print(f\"{'':8}\", end='')\n",
    "for name in student_names:\n",
    "    print(f\"{name:8}\", end='')\n",
    "print()\n",
    "for i, name in enumerate(student_names):\n",
    "    print(f\"{name:8}\", end='')\n",
    "    for j in range(len(student_names)):\n",
    "        print(f\"{cosine_sim[i,j]:8.3f}\", end='')\n",
    "    print()\n",
    "\n",
    "# Find most and least similar pairs\n",
    "def find_extreme_pairs(similarity_matrix, names):\n",
    "    n = len(names)\n",
    "    max_sim = -1\n",
    "    min_sim = 2\n",
    "    max_pair = None\n",
    "    min_pair = None\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):  # Only upper triangle, exclude diagonal\n",
    "            if similarity_matrix[i, j] > max_sim:\n",
    "                max_sim = similarity_matrix[i, j]\n",
    "                max_pair = (names[i], names[j])\n",
    "            if similarity_matrix[i, j] < min_sim:\n",
    "                min_sim = similarity_matrix[i, j]\n",
    "                min_pair = (names[i], names[j])\n",
    "    \n",
    "    return max_pair, max_sim, min_pair, min_sim\n",
    "\n",
    "max_pair, max_sim, min_pair, min_sim = find_extreme_pairs(cosine_sim, student_names)\n",
    "\n",
    "print(f\"\\nSimilarity Analysis:\")\n",
    "print(f\"Most similar pair: {max_pair[0]} & {max_pair[1]} (similarity: {max_sim:.3f})\")\n",
    "print(f\"Least similar pair: {min_pair[0]} & {min_pair[1]} (similarity: {min_sim:.3f})\")\n",
    "\n",
    "# Visualize similarity matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, (sim_matrix, title) in enumerate([(cosine_sim, 'Cosine'),\n",
    "                                         (correlation_sim, 'Correlation'),\n",
    "                                         (euclidean_sim, 'Euclidean')]):\n",
    "    im = axes[i].imshow(sim_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    axes[i].set_title(f'{title} Similarity')\n",
    "    axes[i].set_xticks(range(len(student_names)))\n",
    "    axes[i].set_yticks(range(len(student_names)))\n",
    "    axes[i].set_xticklabels(student_names, rotation=45)\n",
    "    axes[i].set_yticklabels(student_names)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for x in range(len(student_names)):\n",
    "        for y in range(len(student_names)):\n",
    "            axes[i].text(x, y, f'{sim_matrix[y,x]:.2f}', \n",
    "                        ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Team formation based on similarity\n",
    "def form_diverse_teams(similarity_matrix, names, team_size=2):\n",
    "    \\\"\\\"\\\"Form teams to maximize diversity (minimize similarity)\\\"\\\"\\\"\n",
    "    # TODO: Implement team formation algorithm\n",
    "    # For now, find the pair with lowest similarity\n",
    "    \n",
    "    min_sim_idx = np.unravel_index(np.argmin(similarity_matrix + np.eye(len(names))*2), \n",
    "                                   similarity_matrix.shape)\n",
    "    team = [names[min_sim_idx[0]], names[min_sim_idx[1]]]\n",
    "    \n",
    "    return team\n",
    "\n",
    "diverse_team = form_diverse_teams(cosine_sim, student_names)\n",
    "print(f\"\\nTeam Formation:\")\n",
    "print(f\"Most diverse team: {diverse_team} (complementary skills)\")\n",
    "\n",
    "# Recommendation system\n",
    "def recommend_study_partner(target_student, similarity_matrix, names, exclude_self=True):\n",
    "    \\\"\\\"\\\"Recommend study partner based on similarity\\\"\\\"\\\"\n",
    "    target_idx = names.index(target_student)\n",
    "    similarities = similarity_matrix[target_idx].copy()\n",
    "    \n",
    "    if exclude_self:\n",
    "        similarities[target_idx] = -1  # Exclude self\n",
    "    \n",
    "    best_match_idx = np.argmax(similarities)\n",
    "    return names[best_match_idx], similarities[best_match_idx]\n",
    "\n",
    "for student in student_names:\n",
    "    partner, sim_score = recommend_study_partner(student, cosine_sim, student_names)\n",
    "    print(f\"{student}'s best study partner: {partner} (similarity: {sim_score:.3f})\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ ML Applications:\")\n",
    "print(f\"â€¢ Recommendation systems: Find similar users/items\")\n",
    "print(f\"â€¢ Clustering: Group similar data points\")\n",
    "print(f\"â€¢ Collaborative filtering: 'Users like you also liked...'\")\n",
    "print(f\"â€¢ Feature selection: Remove highly correlated features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Exercises: Deeper Applications\n",
    "\n",
    "These exercises combine multiple concepts and introduce more advanced techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.11: ðŸ”´ Custom Distance Metrics for ML\n",
    "#### Learning Objective: Design and implement domain-specific distance measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Text document vectors (word frequencies)\n",
    "documents = {\n",
    "    'doc1': np.array([3, 5, 2, 1, 0]),  # [science, tech, politics, sports, arts]\n",
    "    'doc2': np.array([4, 4, 1, 0, 1]),\n",
    "    'doc3': np.array([0, 1, 5, 3, 2]),\n",
    "    'doc4': np.array([1, 2, 0, 6, 1]),\n",
    "    'doc5': np.array([2, 3, 1, 1, 4])\n",
    "}\n",
    "\n",
    "categories = ['Science', 'Technology', 'Politics', 'Sports', 'Arts']\n",
    "doc_names = list(documents.keys())\n",
    "doc_vectors = np.array(list(documents.values()))\n",
    "\n",
    "print(\"Document Word Frequency Vectors:\")\n",
    "for name, vector in documents.items():\n",
    "    print(f\"{name}: {vector} {categories}\")\n",
    "\n",
    "class CustomDistanceMetrics:\n",
    "    \\\"\\\"\\\"Collection of custom distance metrics for different applications\\\"\\\"\\\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def weighted_euclidean(x, y, weights):\n",
    "        \\\"\\\"\\\"Euclidean distance with feature importance weights\\\"\\\"\\\"\n",
    "        # TODO: Implement weighted Euclidean distance\n",
    "        # Formula: sqrt(sum(w_i * (x_i - y_i)^2))\n",
    "        diff = # Your code here\n",
    "        weighted_diff_sq = # Your code here\n",
    "        distance = # Your code here\n",
    "        return distance\n",
    "    \n",
    "    @staticmethod\n",
    "    def canberra_distance(x, y):\n",
    "        \\\"\\\"\\\"Canberra distance - sensitive to small changes near zero\\\"\\\"\\\"\n",
    "        # TODO: Implement Canberra distance\n",
    "        # Formula: sum(|x_i - y_i| / (|x_i| + |y_i|))\n",
    "        # Handle division by zero with small epsilon\n",
    "        epsilon = 1e-10\n",
    "        numerator = # Your code here\n",
    "        denominator = # Your code here\n",
    "        distance = # Your code here\n",
    "        return distance\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine_distance(x, y):\n",
    "        \\\"\\\"\\\"Cosine distance = 1 - cosine_similarity\\\"\\\"\\\"\n",
    "        # TODO: Implement cosine distance\n",
    "        cos_sim = # Your code here\n",
    "        cos_dist = # Your code here\n",
    "        return cos_dist\n",
    "    \n",
    "    @staticmethod\n",
    "    def earth_movers_distance(x, y):\n",
    "        \\\"\\\"\\\"Simplified Earth Mover's Distance (1D case)\\\"\\\"\\\"\n",
    "        # TODO: Implement simplified EMD\n",
    "        # For vectors, this is the cumulative difference\n",
    "        x_cum = np.cumsum(x / np.sum(x))  # Normalize to probability\n",
    "        y_cum = np.cumsum(y / np.sum(y))\n",
    "        emd = # Your code here\n",
    "        return emd\n",
    "\n",
    "# Define importance weights for documents (some topics more important)\n",
    "topic_weights = np.array([1.5, 1.5, 1.0, 0.8, 0.8])  # Science/Tech more important\n",
    "\n",
    "# Test custom distance metrics\n",
    "metrics = CustomDistanceMetrics()\n",
    "\n",
    "print(f\"\\nDistance Analysis between doc1 and doc2:\")\n",
    "print(f\"Euclidean: {np.linalg.norm(doc_vectors[0] - doc_vectors[1]):.3f}\")\n",
    "print(f\"Weighted Euclidean: {metrics.weighted_euclidean(doc_vectors[0], doc_vectors[1], topic_weights):.3f}\")\n",
    "print(f\"Canberra: {metrics.canberra_distance(doc_vectors[0], doc_vectors[1]):.3f}\")\n",
    "print(f\"Cosine Distance: {metrics.cosine_distance(doc_vectors[0], doc_vectors[1]):.3f}\")\n",
    "print(f\"Earth Mover's: {metrics.earth_movers_distance(doc_vectors[0], doc_vectors[1]):.3f}\")\n",
    "\n",
    "# Create distance matrices for different metrics\n",
    "def create_distance_matrix(vectors, distance_func, **kwargs):\n",
    "    \\\"\\\"\\\"Create pairwise distance matrix using given distance function\\\"\\\"\\\"\n",
    "    n = len(vectors)\n",
    "    matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                matrix[i, j] = 0\n",
    "            else:\n",
    "                matrix[i, j] = distance_func(vectors[i], vectors[j], **kwargs)\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "# Compute distance matrices\n",
    "euclidean_dist = create_distance_matrix(doc_vectors, lambda x, y: np.linalg.norm(x - y))\n",
    "weighted_dist = create_distance_matrix(doc_vectors, metrics.weighted_euclidean, weights=topic_weights)\n",
    "canberra_dist = create_distance_matrix(doc_vectors, metrics.canberra_distance)\n",
    "cosine_dist = create_distance_matrix(doc_vectors, metrics.cosine_distance)\n",
    "\n",
    "# Visualize distance matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "distance_matrices = [euclidean_dist, weighted_dist, canberra_dist, cosine_dist]\n",
    "titles = ['Euclidean', 'Weighted Euclidean', 'Canberra', 'Cosine']\n",
    "\n",
    "for i, (dist_matrix, title) in enumerate(zip(distance_matrices, titles)):\n",
    "    im = axes[i].imshow(dist_matrix, cmap='viridis')\n",
    "    axes[i].set_title(f'{title} Distance')\n",
    "    axes[i].set_xticks(range(len(doc_names)))\n",
    "    axes[i].set_yticks(range(len(doc_names)))\n",
    "    axes[i].set_xticklabels(doc_names)\n",
    "    axes[i].set_yticklabels(doc_names)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for x in range(len(doc_names)):\n",
    "        for y in range(len(doc_names)):\n",
    "            axes[i].text(x, y, f'{dist_matrix[y,x]:.2f}', \n",
    "                        ha='center', va='center', fontsize=8, color='white')\n",
    "    \n",
    "    plt.colorbar(im, ax=axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Document clustering based on different metrics\n",
    "def find_closest_documents(distance_matrix, doc_names):\n",
    "    \\\"\\\"\\\"Find most similar document pairs\\\"\\\"\\\"\n",
    "    n = len(doc_names)\n",
    "    min_dist = float('inf')\n",
    "    closest_pair = None\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if distance_matrix[i, j] < min_dist:\n",
    "                min_dist = distance_matrix[i, j]\n",
    "                closest_pair = (doc_names[i], doc_names[j])\n",
    "    \n",
    "    return closest_pair, min_dist\n",
    "\n",
    "print(f\"\\nClosest Document Pairs by Different Metrics:\")\n",
    "for dist_matrix, metric_name in zip(distance_matrices, titles):\n",
    "    pair, distance = find_closest_documents(dist_matrix, doc_names)\n",
    "    print(f\"{metric_name:18}: {pair[0]} & {pair[1]} (distance: {distance:.3f})\")\n",
    "\n",
    "# Performance analysis: which metric works best for document similarity?\n",
    "def evaluate_metric_consistency(distance_matrices, doc_vectors):\n",
    "    \\\"\\\"\\\"Evaluate how consistent different metrics are\\\"\\\"\\\"\n",
    "    n_metrics = len(distance_matrices)\n",
    "    consistency_scores = np.zeros((n_metrics, n_metrics))\n",
    "    \n",
    "    for i in range(n_metrics):\n",
    "        for j in range(n_metrics):\n",
    "            # Calculate rank correlation between distance matrices\n",
    "            rank_corr = np.corrcoef(distance_matrices[i].flatten(), \n",
    "                                   distance_matrices[j].flatten())[0, 1]\n",
    "            consistency_scores[i, j] = rank_corr\n",
    "    \n",
    "    return consistency_scores\n",
    "\n",
    "consistency = evaluate_metric_consistency(distance_matrices, doc_vectors)\n",
    "\n",
    "print(f\"\\nMetric Consistency Analysis (Correlation):\")\n",
    "print(f\"{'':18}\", end='')\n",
    "for title in titles:\n",
    "    print(f\"{title:12}\", end='')\n",
    "print()\n",
    "for i, title in enumerate(titles):\n",
    "    print(f\"{title:18}\", end='')\n",
    "    for j in range(len(titles)):\n",
    "        print(f\"{consistency[i,j]:12.3f}\", end='')\n",
    "    print()\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Choosing the Right Metric:\")\n",
    "print(f\"â€¢ Euclidean: General purpose, treats all features equally\")\n",
    "print(f\"â€¢ Weighted Euclidean: When features have different importance\")\n",
    "print(f\"â€¢ Canberra: Sensitive to small changes, good for sparse data\")\n",
    "print(f\"â€¢ Cosine: Ignores magnitude, focuses on direction/proportion\")\n",
    "print(f\"â€¢ Earth Mover's: Considers feature order/structure\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Design Challenge:\")\n",
    "print(f\"If you were building a document search engine, which metric would\")\n",
    "print(f\"you choose and why? Consider the trade-offs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Matrices & Operations\n",
    "\n",
    "### What are Matrices in ML?\n",
    "- **Data Matrix**: Rows = samples, Columns = features\n",
    "- **Weight Matrix**: Parameters in neural networks\n",
    "- **Covariance Matrix**: Relationships between features\n",
    "- **Transformation Matrix**: PCA, rotations, scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Student dataset matrix\n",
    "# Each row is a student, columns are [height, weight, age, grade]\n",
    "students_data = np.array([\n",
    "    [170, 65, 20, 85],  # Student 1\n",
    "    [180, 75, 22, 90],  # Student 2\n",
    "    [165, 60, 19, 88],  # Student 3\n",
    "    [175, 70, 21, 92],  # Student 4\n",
    "    [185, 80, 23, 87]   # Student 5\n",
    "])\n",
    "\n",
    "print(f\"Data matrix shape: {students_data.shape}\")\n",
    "print(f\"Number of students: {students_data.shape[0]}\")\n",
    "print(f\"Number of features: {students_data.shape[1]}\")\n",
    "print(f\"\\nData matrix:\")\n",
    "print(students_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Operations\n",
    "\n",
    "#### 1. Matrix Multiplication\n",
    "**Rule**: $(A \\times B)_{ij} = \\sum_{k} A_{ik} B_{kj}$\n",
    "\n",
    "**Requirement**: columns of A = rows of B\n",
    "\n",
    "**ML Application**: Computing predictions, transforming features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1: Linear regression prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict grade based on [height, weight, age]\n",
    "\n",
    "# Features matrix (without grade column)\n",
    "X = students_data[:, :3]  # height, weight, age\n",
    "y = students_data[:, 3]   # grade\n",
    "\n",
    "# Simple weight matrix for linear combination\n",
    "# Weights: [0.1 for height, 0.2 for weight, 1.0 for age]\n",
    "weights = np.array([0.1, 0.2, 1.0]).reshape(-1, 1)\n",
    "\n",
    "print(f\"Feature matrix X shape: {X.shape}\")\n",
    "print(f\"Weight vector shape: {weights.shape}\")\n",
    "\n",
    "# Matrix multiplication: X @ weights\n",
    "predictions = X @ weights\n",
    "print(f\"\\nPredictions shape: {predictions.shape}\")\n",
    "print(f\"Predictions: {predictions.flatten()}\")\n",
    "print(f\"Actual grades: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Matrix Transpose\n",
    "**Operation**: Flip matrix along diagonal (rows â†” columns)\n",
    "\n",
    "**ML Applications**: \n",
    "- Computing gradients\n",
    "- Changing data layout\n",
    "- Solving normal equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2: Matrix transpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original matrix shape: {X.shape}\")\n",
    "print(f\"Original matrix:\")\n",
    "print(X)\n",
    "\n",
    "# Transpose\n",
    "X_T = X.T\n",
    "print(f\"\\nTransposed shape: {X_T.shape}\")\n",
    "print(f\"Transposed matrix:\")\n",
    "print(X_T)\n",
    "\n",
    "# Common ML operation: X^T @ X (used in normal equation)\n",
    "XTX = X_T @ X\n",
    "print(f\"\\nX^T @ X shape: {XTX.shape}\")\n",
    "print(f\"X^T @ X (covariance-like matrix):\")\n",
    "print(XTX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.3: Feature standardization (Z-score normalization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formula: z = (x - Î¼) / Ïƒ\n",
    "\n",
    "def standardize_features(X):\n",
    "    \"\"\"Standardize features to have mean=0 and std=1\"\"\"\n",
    "    # TODO: Implement standardization\n",
    "    mean = np.mean(X, axis=0)  # Mean of each column\n",
    "    std = np.std(X, axis=0)    # Standard deviation of each column\n",
    "    \n",
    "    # Your code here\n",
    "    X_standardized = # code here\n",
    "    return X_standardized, mean, std\n",
    "\n",
    "# Apply standardization\n",
    "X_std, mean_vals, std_vals = standardize_features(X)\n",
    "\n",
    "print(f\"Original mean: {np.mean(X, axis=0)}\")\n",
    "print(f\"Original std: {np.std(X, axis=0)}\")\n",
    "print(f\"\\nStandardized mean: {np.mean(X_std, axis=0)}\")\n",
    "print(f\"Standardized std: {np.std(X_std, axis=0)}\")\n",
    "\n",
    "# Visualize before and after\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.boxplot(X, labels=['Height', 'Weight', 'Age'])\n",
    "ax1.set_title('Original Features')\n",
    "ax1.set_ylabel('Values')\n",
    "\n",
    "ax2.boxplot(X_std, labels=['Height', 'Weight', 'Age'])\n",
    "ax2.set_title('Standardized Features')\n",
    "ax2.set_ylabel('Z-scores')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.5: ðŸŸ¢ Matrix Addition and Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Learning Objective: Master matrix arithmetic and NumPy broadcasting rules\n",
    "\n",
    "# Sensor network data: multiple sensors recording over time\n",
    "# Each row is a time point, each column is a sensor\n",
    "sensor_readings = np.array([\n",
    "    [22.5, 18.2, 25.1, 20.8],  # Time 1\n",
    "    [23.0, 18.5, 25.3, 21.0],  # Time 2\n",
    "    [22.8, 18.3, 25.0, 20.9],  # Time 3\n",
    "    [23.2, 18.7, 25.5, 21.2]   # Time 4\n",
    "])\n",
    "\n",
    "# Calibration offsets for each sensor (systematic errors)\n",
    "calibration_offsets = np.array([0.5, -0.2, 0.3, -0.1])\n",
    "\n",
    "# Time-based drift (affects all sensors equally over time)\n",
    "time_drift = np.array([[0.1], [0.2], [0.3], [0.4]])\n",
    "\n",
    "print(\"Sensor Network Calibration:\")\n",
    "print(f\"Raw readings shape: {sensor_readings.shape}\")\n",
    "print(f\"Calibration offsets: {calibration_offsets}\")\n",
    "print(f\"Time drift: {time_drift.flatten()}\")\n",
    "\n",
    "# TODO: Basic matrix operations\n",
    "# 1. Add calibration offset to all readings\n",
    "calibrated_readings = # Your code here\n",
    "\n",
    "# 2. Remove time drift from calibrated readings  \n",
    "drift_corrected = # Your code here\n",
    "\n",
    "# 3. Calculate sensor mean and standard deviation\n",
    "sensor_means = # Your code here\n",
    "sensor_stds = # Your code here\n",
    "\n",
    "print(f\"\\nMatrix Operations Results:\")\n",
    "print(f\"Original readings:\\\\n{sensor_readings}\")\n",
    "print(f\"After calibration:\\\\n{calibrated_readings}\")\n",
    "print(f\"After drift correction:\\\\n{drift_corrected}\")\n",
    "print(f\"Sensor means: {sensor_means}\")\n",
    "print(f\"Sensor stds: {sensor_stds}\")\n",
    "\n",
    "# Broadcasting examples\n",
    "print(f\"\\nBroadcasting Examples:\")\n",
    "\n",
    "# Example 1: Add different value to each row\n",
    "row_additions = np.array([1, 2, 3, 4]).reshape(-1, 1)\n",
    "result1 = sensor_readings + row_additions\n",
    "print(f\"Add [1,2,3,4] to each row:\\\\n{result1}\")\n",
    "\n",
    "# Example 2: Multiply each column by different factor\n",
    "col_multipliers = np.array([1.1, 0.9, 1.05, 0.95])\n",
    "result2 = sensor_readings * col_multipliers\n",
    "print(f\"Multiply columns by [1.1, 0.9, 1.05, 0.95]:\\\\n{result2}\")\n",
    "\n",
    "# Example 3: Element-wise operations\n",
    "sensor_squared = sensor_readings ** 2\n",
    "sensor_differences = sensor_readings[1:] - sensor_readings[:-1]\n",
    "print(f\"Squared readings shape: {sensor_squared.shape}\")\n",
    "print(f\"Time differences shape: {sensor_differences.shape}\")\n",
    "\n",
    "# Visualize the corrections\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Plot 1: Raw vs calibrated data\n",
    "axes[0,0].plot(sensor_readings, 'o-', alpha=0.7)\n",
    "axes[0,0].plot(calibrated_readings, 's--', alpha=0.7)\n",
    "axes[0,0].set_title('Raw vs Calibrated Readings')\n",
    "axes[0,0].set_xlabel('Time')\n",
    "axes[0,0].set_ylabel('Temperature')\n",
    "axes[0,0].legend(['Raw S1', 'Raw S2', 'Raw S3', 'Raw S4', \n",
    "                  'Cal S1', 'Cal S2', 'Cal S3', 'Cal S4'])\n",
    "\n",
    "# Plot 2: Time drift effect\n",
    "axes[0,1].plot(sensor_readings.mean(axis=1), 'o-', label='Raw Mean')\n",
    "axes[0,1].plot(drift_corrected.mean(axis=1), 's-', label='Corrected Mean')\n",
    "axes[0,1].set_title('Time Drift Correction')\n",
    "axes[0,1].set_xlabel('Time')\n",
    "axes[0,1].set_ylabel('Average Reading')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Plot 3: Sensor statistics\n",
    "x_pos = np.arange(4)\n",
    "axes[1,0].bar(x_pos - 0.2, sensor_means, 0.4, label='Mean', alpha=0.7)\n",
    "axes[1,0].bar(x_pos + 0.2, sensor_stds, 0.4, label='Std Dev', alpha=0.7)\n",
    "axes[1,0].set_title('Sensor Statistics')\n",
    "axes[1,0].set_xlabel('Sensor')\n",
    "axes[1,0].set_ylabel('Value')\n",
    "axes[1,0].set_xticks(x_pos)\n",
    "axes[1,0].set_xticklabels(['S1', 'S2', 'S3', 'S4'])\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Plot 4: Broadcasting visualization\n",
    "axes[1,1].imshow(result2, cmap='viridis', aspect='auto')\n",
    "axes[1,1].set_title('Column Scaling (Broadcasting)')\n",
    "axes[1,1].set_xlabel('Sensor')\n",
    "axes[1,1].set_ylabel('Time')\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axes[1,1].text(j, i, f'{result2[i,j]:.1f}', ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Challenge: Matrix broadcasting rules\n",
    "print(f\"\\nðŸ§  Broadcasting Challenge:\")\n",
    "print(f\"Predict the output shapes without running the code:\")\n",
    "\n",
    "examples = [\n",
    "    (\"(4,4) + (4,)\", \"sensor_readings + calibration_offsets\"),\n",
    "    (\"(4,4) - (4,1)\", \"sensor_readings - time_drift\"), \n",
    "    (\"(4,4) * (1,4)\", \"sensor_readings * col_multipliers.reshape(1,-1)\"),\n",
    "    (\"(4,4) + (1,)\", \"sensor_readings + 5\"),\n",
    "]\n",
    "\n",
    "for shape_desc, operation in examples:\n",
    "    print(f\"  {shape_desc:12} â†’ {operation}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ ML Applications:\")\n",
    "print(f\"â€¢ Data Preprocessing: Centering and scaling features\")\n",
    "print(f\"â€¢ Batch Processing: Applying operations to mini-batches\")\n",
    "print(f\"â€¢ Neural Networks: Adding biases, applying activations\")\n",
    "print(f\"â€¢ Feature Engineering: Creating interaction terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Eigenvalues & Eigenvectors (25 minutes)\n",
    "\n",
    "### Intuitive Understanding\n",
    "**Eigenvector**: A direction that doesn't change when a matrix transformation is applied\n",
    "\n",
    "**Eigenvalue**: How much the eigenvector is stretched/shrunk\n",
    "\n",
    "**Equation**: $A \\vec{v} = \\lambda \\vec{v}$\n",
    "- $A$: matrix\n",
    "- $\\vec{v}$: eigenvector \n",
    "- $\\lambda$: eigenvalue\n",
    "\n",
    "### ML Applications\n",
    "- **PCA**: Principal directions of data variance\n",
    "- **Spectral clustering**: Graph partitioning\n",
    "- **Stability analysis**: System behavior\n",
    "\n",
    "Watch: https://www.youtube.com/watch?v=PFDu9oVAE-g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.1: Compute eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using covariance matrix of our student data\n",
    "\n",
    "# Calculate covariance matrix\n",
    "cov_matrix = np.cov(X_std.T)  # Transpose because np.cov expects features as rows\n",
    "print(f\"Covariance matrix shape: {cov_matrix.shape}\")\n",
    "print(f\"Covariance matrix:\")\n",
    "print(cov_matrix)\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "print(f\"\\nEigenvalues: {eigenvalues}\")\n",
    "print(f\"Eigenvectors shape: {eigenvectors.shape}\")\n",
    "print(f\"Eigenvectors (columns are eigenvectors):\")\n",
    "print(eigenvectors)\n",
    "\n",
    "# Sort by eigenvalue magnitude (descending)\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "print(f\"\\nSorted eigenvalues: {sorted_eigenvalues}\")\n",
    "print(f\"Explained variance ratio: {sorted_eigenvalues / np.sum(sorted_eigenvalues)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.3: Create and compress a simple \"image\"\n",
    "# Create a synthetic 2D pattern that represents an image\n",
    "\n",
    "# Generate a simple pattern\n",
    "x = np.linspace(0, 4*np.pi, 50)\n",
    "y = np.linspace(0, 4*np.pi, 50)\n",
    "X_grid, Y_grid = np.meshgrid(x, y)\n",
    "\n",
    "# Create pattern: combination of sine waves\n",
    "image = np.sin(X_grid) * np.cos(Y_grid) + 0.5 * np.sin(2*X_grid + Y_grid)\n",
    "\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "print(f\"Original image size: {image.size} values\")\n",
    "\n",
    "# Apply SVD compression\n",
    "def compress_image(img, k):\n",
    "    U, s, VT = np.linalg.svd(img, full_matrices=False)\n",
    "    compressed = U[:, :k] @ np.diag(s[:k]) @ VT[:k, :]\n",
    "    \n",
    "    # Calculate compression statistics\n",
    "    original_size = img.size\n",
    "    compressed_size = k * (U.shape[0] + VT.shape[1]) + k\n",
    "    compression_ratio = compressed_size / original_size\n",
    "    mse = np.mean((img - compressed) ** 2)\n",
    "    \n",
    "    return compressed, compression_ratio, mse\n",
    "\n",
    "# Test different compression levels\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Original\n",
    "im0 = axes[0].imshow(image, cmap='coolwarm')\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Different compression levels\n",
    "k_values = [1, 3, 5, 10, 20]\n",
    "for i, k in enumerate(k_values):\n",
    "    compressed, comp_ratio, mse = compress_image(image, k)\n",
    "    \n",
    "    im = axes[i+1].imshow(compressed, cmap='coolwarm')\n",
    "    axes[i+1].set_title(f'k={k}\\nRatio: {comp_ratio:.2f}\\nMSE: {mse:.3f}')\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show compression trade-off curve\n",
    "k_range = range(1, min(image.shape) + 1)\n",
    "compression_ratios = []\n",
    "mse_values = []\n",
    "\n",
    "for k in k_range:\n",
    "    _, ratio, mse = compress_image(image, k)\n",
    "    compression_ratios.append(ratio)\n",
    "    mse_values.append(mse)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, compression_ratios, 'b-', linewidth=2)\n",
    "plt.xlabel('Number of Components (k)')\n",
    "plt.ylabel('Compression Ratio')\n",
    "plt.title('Storage Requirements')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_range, mse_values, 'r-', linewidth=2)\n",
    "plt.xlabel('Number of Components (k)')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Reconstruction Error')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Key Takeaways\n",
    "\n",
    "### What We Covered\n",
    "\n",
    "| Concept | Key Formula | ML Application |\n",
    "|---------|-------------|----------------|\n",
    "| **Dot Product** | $\\vec{a} \\cdot \\vec{b} = \\sum a_i b_i$ | Similarity, Neural networks |\n",
    "| **Norms** | $\\|\\vec{v}\\|_2 = \\sqrt{\\sum v_i^2}$ | Distance, Regularization |\n",
    "| **Matrix Multiplication** | $(AB)_{ij} = \\sum_k A_{ik}B_{kj}$ | Linear transformations |\n",
    "| **Eigendecomposition** | $A\\vec{v} = \\lambda\\vec{v}$ | PCA, Spectral methods |\n",
    "\n",
    "### Essential NumPy Functions\n",
    "```python\n",
    "# Vectors\n",
    "np.dot(a, b)              # Dot product\n",
    "np.linalg.norm(v)         # Vector norm\n",
    "a @ b                     # Matrix multiplication\n",
    "\n",
    "# Matrices  \n",
    "A.T                       # Transpose\n",
    "np.linalg.inv(A)         # Inverse\n",
    "np.linalg.eig(A)         # Eigenvalues/vectors\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
